import nest_asyncio
nest_asyncio.apply()
# app.py (PhiÃªn báº£n chÃ­nh xÃ¡c cho LangChain v0.2.x trá»Ÿ lÃªn)
import os
from dotenv import load_dotenv
import streamlit as st
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_pinecone import PineconeVectorStore # <<< THAY Äá»”I QUAN TRá»ŒNG
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

load_dotenv()

# --- PHáº¦N LOGIC (Cáº­p nháº­t hoÃ n toÃ n theo cÃº phÃ¡p má»›i) ---
def get_vectorstore():
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
    # Láº¥y tÃªn index tá»« biáº¿n mÃ´i trÆ°á»ng, náº¿u khÃ´ng cÃ³ thÃ¬ dÃ¹ng tÃªn máº·c Ä‘á»‹nh
    index_name = os.getenv("PINECONE_INDEX_NAME", "my-chatbot-index-gemini")
    vectorstore = PineconeVectorStore.from_existing_index(
        index_name=index_name, 
        embedding=embeddings
    )
    return vectorstore

def get_context_retriever_chain(vector_store):
    # Model nÃ y chá»‰ dÃ¹ng Ä‘á»ƒ táº¡o cÃ¢u há»i tÃ¬m kiáº¿m, cÃ³ thá»ƒ dÃ¹ng model nhanh hÆ¡n
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite-preview-06-17", temperature=0, convert_system_message_to_human=True)
    retriever = vector_store.as_retriever()
    
    # PROMPT ÄÃšNG CHO VIá»†C Táº O CÃ‚U Há»ŽI TÃŒM KIáº¾M
    # NÃ³ chá»‰ nháº­n vÃ o 'chat_history' vÃ  'input', khÃ´ng cÃ³ 'context'
    prompt = ChatPromptTemplate.from_messages([
      MessagesPlaceholder(variable_name="chat_history"),
      ("user", "{input}"),
      ("user", "Dá»±a vÃ o cuá»™c há»™i thoáº¡i trÃªn, hÃ£y táº¡o ra má»™t cÃ¢u há»i tÃ¬m kiáº¿m Ä‘á»™c láº­p Ä‘á»ƒ cÃ³ thá»ƒ tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i cuá»‘i cÃ¹ng cá»§a ngÆ°á»i dÃ¹ng.")
    ])
    
    retriever_chain = create_history_aware_retriever(llm, retriever, prompt)
    return retriever_chain

def get_conversational_rag_chain(retriever_chain):
    # Model nÃ y dÃ¹ng Ä‘á»ƒ suy luáº­n vÃ  tráº£ lá»i, nÃªn dÃ¹ng model máº¡nh hÆ¡n
    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite-preview-06-17", temperature=0.7, convert_system_message_to_human=True)
    
    # PROMPT ÄÃšNG CHO VIá»†C Táº O CÃ‚U TRáº¢ Lá»œI CUá»I CÃ™NG
    # Prompt nÃ y má»›i lÃ  nÆ¡i cáº§n biáº¿n 'context'
    system_prompt = (
        "Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p, nhiá»‡m vá»¥ cá»§a báº¡n lÃ  tráº£ lá»i cÃ¡c cÃ¢u há»i vá» tÃ i liá»‡u ná»™i bá»™. "
        "HÃ£y sá»­ dá»¥ng cÃ¡c Ä‘oáº¡n vÄƒn báº£n Ä‘Æ°á»£c cung cáº¥p dÆ°á»›i Ä‘Ã¢y Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng. "
        "Tráº£ lá»i má»™t cÃ¡ch ngáº¯n gá»n, chÃ­nh xÃ¡c vÃ  Ä‘i tháº³ng vÃ o váº¥n Ä‘á». "
        "Náº¿u cÃ¡c Ä‘oáº¡n vÄƒn báº£n khÃ´ng chá»©a thÃ´ng tin Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i, hÃ£y nÃ³i rÃµ rÃ ng ráº±ng: "
        "'TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y trong tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.' "
        "Tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘Æ°á»£c bá»‹a ra thÃ´ng tin khÃ´ng cÃ³ trong vÄƒn báº£n. "
        "HÃ£y tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.\n\n"
        "Ná»™i dung vÄƒn báº£n:\n{context}"
    )
    
    prompt = ChatPromptTemplate.from_messages([
      ("system", system_prompt),
      MessagesPlaceholder(variable_name="chat_history"),
      ("user", "{input}"),
    ])
    
    stuff_documents_chain = create_stuff_documents_chain(llm, prompt)
    return create_retrieval_chain(retriever_chain, stuff_documents_chain)

# --- PHáº¦N GIAO DIá»†N STREAMLIT ---
def main():
    st.set_page_config(page_title="Chat vá»›i tÃ i liá»‡u cá»§a báº¡n", page_icon=":books:")
    st.header("Chat vá»›i tÃ i liá»‡u cá»§a báº¡n (dÃ¹ng Gemini) :sparkles:")
    
    ADMIN_KEYWORD = "key_admin"

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # --- Táº M THá»œI Bá»Ž CACHE VÃ€ THÃŠM LOG Äá»‚ DEBUG ---
    st.write("Báº¯t Ä‘áº§u khá»Ÿi táº¡o cÃ¡c thÃ nh pháº§n...")

    try:
        vector_store = get_vectorstore()
        st.write("âœ… ÄÃ£ káº¿t ná»‘i Vector Store thÃ nh cÃ´ng.")
        
        retriever_chain = get_context_retriever_chain(vector_store)
        st.write("âœ… ÄÃ£ táº¡o Retriever Chain thÃ nh cÃ´ng.")
        
        conversation_rag_chain = get_conversational_rag_chain(retriever_chain)
        st.write("âœ… ÄÃ£ táº¡o RAG Chain thÃ nh cÃ´ng.")
        
        admin_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash-lite-preview-06-17", temperature=0.7)
        st.write("âœ… ÄÃ£ khá»Ÿi táº¡o Admin LLM thÃ nh cÃ´ng.")
        
        st.success("Táº¥t cáº£ cÃ¡c thÃ nh pháº§n Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng!")

    except Exception as e:
        st.error(f"ÄÃ£ xáº£y ra lá»—i trong quÃ¡ trÃ¬nh khá»Ÿi táº¡o: {e}")
        # Dá»«ng á»©ng dá»¥ng náº¿u cÃ³ lá»—i khá»Ÿi táº¡o
        return

    # --- PHáº¦N CÃ’N Láº I Cá»¦A HÃ€M MAIN GIá»® NGUYÃŠN ---
    
    # Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
    for message in st.session_state.chat_history:
        role = "user" if isinstance(message, HumanMessage) else "assistant"
        with st.chat_message(role):
            st.markdown(message.content)

    user_question = st.chat_input("HÃ£y há»i tÃ´i báº¥t cá»© Ä‘iá»u gÃ¬ vá» tÃ i liá»‡u cá»§a báº¡n:")

    if user_question:
        # ThÃªm cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng vÃ o lá»‹ch sá»­ vÃ  hiá»ƒn thá»‹ ngay láº­p tá»©c
        st.session_state.chat_history.append(HumanMessage(content=user_question))
        with st.chat_message("user"):
            st.markdown(user_question)

        # Hiá»ƒn thá»‹ icon "Ä‘ang suy nghÄ©"
        with st.spinner("Gemini Ä‘ang suy nghÄ©..."):
            
            # --- LOGIC CHUYá»‚N Äá»”I CHáº¾ Äá»˜ ---
            if ADMIN_KEYWORD in user_question.lower():
                st.info(f"ÄÃ£ kÃ­ch hoáº¡t cháº¿ Ä‘á»™ Admin! Bá» qua ngá»¯ cáº£nh tÃ i liá»‡u.", icon="ðŸ”‘")
                
                # XÃ³a tá»« khÃ³a khá»i cÃ¢u há»i Ä‘á»ƒ khÃ´ng lÃ m nhiá»…u model
                actual_question = user_question.lower().replace(ADMIN_KEYWORD, "").strip()
                
                # Táº¡o prompt Ä‘Æ¡n giáº£n cho cháº¿ Ä‘á»™ chat thÃ´ng thÆ°á»ng
                admin_prompt = ChatPromptTemplate.from_messages([
                    ("system", "Báº¡n lÃ  má»™t trá»£ lÃ½ AI há»¯u Ã­ch. HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch toÃ n diá»‡n."),
                    MessagesPlaceholder(variable_name="chat_history"),
                    ("user", "{input}")
                ])
                
                # Táº¡o má»™t chuá»—i xá»­ lÃ½ Ä‘Æ¡n giáº£n chá»‰ gá»“m prompt vÃ  LLM
                admin_chain = admin_prompt | admin_llm
                
                response = admin_chain.invoke({
                    "chat_history": st.session_state.chat_history,
                    "input": actual_question
                })
                # Láº¥y ná»™i dung tá»« response cá»§a model
                bot_response = response.content

            else: # Cháº¿ Ä‘á»™ RAG máº·c Ä‘á»‹nh
                response = conversation_rag_chain.invoke({
                    "chat_history": st.session_state.chat_history,
                    "input": user_question
                })
                bot_response = response['answer']
                
                # --- TÃNH NÄ‚NG DEBUG NGá»® Cáº¢NH NÃ‚NG CAO ---
                with st.expander("Xem chi tiáº¿t quÃ¡ trÃ¬nh truy xuáº¥t", expanded=False):
                    # Láº¥y danh sÃ¡ch cÃ¡c tÃ i liá»‡u nguá»“n tá»« context
                    source_documents = response.get('context', [])
                    
                    # Äáº¿m sá»‘ lÆ°á»£ng chunk
                    num_chunks = len(source_documents)
                    
                    # Hiá»ƒn thá»‹ thÃ´ng bÃ¡o
                    st.info(f"ÄÃ£ truy xuáº¥t Ä‘Æ°á»£c **{num_chunks} chunk** tá»« Pinecone Ä‘á»ƒ lÃ m ngá»¯ cáº£nh.", icon="â„¹ï¸")
                    
                    st.write("---") # ThÃªm má»™t Ä‘Æ°á»ng káº» phÃ¢n cÃ¡ch

                    # Láº·p qua vÃ  hiá»ƒn thá»‹ tá»«ng chunk
                    for i, doc in enumerate(source_documents):
                        st.subheader(f"Chunk #{i + 1}")
                        
                        # Cá»‘ gáº¯ng láº¥y tÃªn file tá»« metadata
                        source = doc.metadata.get('source', 'KhÃ´ng rÃµ nguá»“n')
                        file_name = os.path.basename(source)
                        st.write(f"**Nguá»“n:** `{file_name}`")
                        
                        # Hiá»ƒn thá»‹ ná»™i dung cá»§a chunk
                        st.text_area(
                            label=f"Ná»™i dung chunk {i + 1}", 
                            value=doc.page_content, 
                            height=200, 
                            key=f"chunk_{i}" # Key duy nháº¥t cho má»—i text_area
                        )
                        st.write("---")

        # ThÃªm cÃ¢u tráº£ lá»i cá»§a bot vÃ o lá»‹ch sá»­ vÃ  hiá»ƒn thá»‹
        st.session_state.chat_history.append(AIMessage(content=bot_response))
        with st.chat_message("assistant"):
            st.markdown(bot_response)

if __name__ == '__main__':
    main()